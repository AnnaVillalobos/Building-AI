{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1\n",
    "## 1.2 Optimization\n",
    "### Exercise 1: Listing pineapple routes\n",
    "In this exercise you need to list all the possible routes that start from Panama and visit each of the other ports exactly once.\n",
    "\n",
    "The template code below contains an incomplete permutations function which takes as input a specified route and a list of port names absent from that route. Modify the function so that it prints out all the possible orderings of the ports that always begin with Panama (PAN).\n",
    "\n",
    "The mathematical term for such orderings is a permutation. Note that your program should work for an input portnames list of any length. The order in which the permutations are printed doesn't matter.\n",
    "\n",
    "As the output the function should print each permutation on its own row, as one string, with the port names separated by spaces. For this, you can use the join function as follows: print(' '.join([portnames[i] for i in route]))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAN AMS CAS NYC HEL\n",
      "PAN AMS CAS HEL NYC\n",
      "PAN AMS NYC CAS HEL\n",
      "PAN AMS NYC HEL CAS\n",
      "PAN AMS HEL CAS NYC\n",
      "PAN AMS HEL NYC CAS\n",
      "PAN CAS AMS NYC HEL\n",
      "PAN CAS AMS HEL NYC\n",
      "PAN CAS NYC AMS HEL\n",
      "PAN CAS NYC HEL AMS\n",
      "PAN CAS HEL AMS NYC\n",
      "PAN CAS HEL NYC AMS\n",
      "PAN NYC AMS CAS HEL\n",
      "PAN NYC AMS HEL CAS\n",
      "PAN NYC CAS AMS HEL\n",
      "PAN NYC CAS HEL AMS\n",
      "PAN NYC HEL AMS CAS\n",
      "PAN NYC HEL CAS AMS\n",
      "PAN HEL AMS CAS NYC\n",
      "PAN HEL AMS NYC CAS\n",
      "PAN HEL CAS AMS NYC\n",
      "PAN HEL CAS NYC AMS\n",
      "PAN HEL NYC AMS CAS\n",
      "PAN HEL NYC CAS AMS\n"
     ]
    }
   ],
   "source": [
    "portnames = [\"PAN\", \"AMS\", \"CAS\", \"NYC\", \"HEL\"]\n",
    " \n",
    "def permutations(route, ports):\n",
    "    # Write your recursive code here\n",
    "    for i in range(len(ports)):\n",
    "        new_route = route + [ports[i]]\n",
    "        new_ports = ports[:i] + ports[i+1:]\n",
    "        permutations(new_route, new_ports)\n",
    "\n",
    "    # Print the port names in route when the recursion terminates\n",
    "\n",
    "    if not ports:\n",
    "        print(' '.join([portnames[i] for i in route]))\n",
    "        return\n",
    "\n",
    "\n",
    "# This will start the recursion with 0 (\"PAN\") as the first stop\n",
    "permutations([0], list(range(1, len(portnames))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Pineapple route emissions\n",
    "Building on the previous solution, modify the code so that it finds the route with minimum carbon emissions and prints it out. Again, the program should work for any number of ports. You can assume that the distances between the ports are given in an array of the appropriate size so that the distance between ports i and j is found in D[i][j]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAN NYC CAS AMS HEL 283.7 kg\n"
     ]
    }
   ],
   "source": [
    "portnames = [\"PAN\", \"AMS\", \"CAS\", \"NYC\", \"HEL\"]\n",
    "\n",
    "# https://sea-distances.org/\n",
    "# nautical miles converted to km\n",
    "\n",
    "D = [\n",
    "        [0,8943,8019,3652,10545],\n",
    "        [8943,0,2619,6317,2078],\n",
    "        [8019,2619,0,5836,4939],\n",
    "        [3652,6317,5836,0,7825],\n",
    "        [10545,2078,4939,7825,0]\n",
    "    ]\n",
    "\n",
    "# https://timeforchange.org/co2-emissions-shipping-goods\n",
    "# assume 20g per km per metric ton (of pineapples)\n",
    "\n",
    "co2 = 0.020\n",
    "\n",
    "# DATA BLOCK ENDS\n",
    "\n",
    "# these variables are initialised to nonsensical values\n",
    "# your program should determine the correct values for them\n",
    "smallest = float(\"inf\")\n",
    "bestroute = []\n",
    "\n",
    "def permutations(route, ports):\n",
    "    # write the recursive function here\n",
    "    # remember to calculate the emissions of the route as the recursion ends\n",
    "    # and keep track of the route with the lowest emissions\n",
    "    global smallest, bestroute\n",
    "\n",
    "    if not ports:\n",
    "        pollution = 0\n",
    "        for i in range(len(route) - 1):\n",
    "            pollution += D[route[i]][route[i + 1]] * co2\n",
    "        \n",
    "        if pollution < smallest:\n",
    "            smallest = pollution\n",
    "            bestroute = route\n",
    "            return\n",
    "\n",
    "    for i in range(len(ports)):\n",
    "        permutations(route + [ports[i]], ports[:i] + ports[i + 1:])\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    # Do not edit any (global) variables using this function, as it will mess up the testing\n",
    "\n",
    "    # this will start the recursion \n",
    "    permutations([0], list(range(1, len(portnames))))\n",
    "\n",
    "    # print the best route and its emissions\n",
    "    print(' '.join([portnames[i] for i in bestroute]) + \" %.1f kg\" % smallest)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Hill climbing\n",
    "### Exercise 3: Reach the highest summit\n",
    "Let the elevation at each point on the mountain be stored in array h of size 100. The elevation at the leftmost point is thus stored in h[0] and the elevation at the rightmost point is stored in h[99].\n",
    "\n",
    "The following program starts at a random position and keeps going to the right until Venla can no longer go up. However, perhaps the mountain is a bit rugged which means it's necessary to look a bit further ahead.\n",
    "\n",
    "Edit the program so that Venla doesn't stop climbing as long as she can go up by moving up to five steps either left or right. If there are multiple choices within five steps that go up, any one of them is good. To check how your climbing algorithm works in action, you can plot the results of your hill climbing using the Plot button. As a reminder, the summit will be marked with a blue triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is  1.235891613012179\n",
      "x_new is  1.2773542213111282\n",
      "x is  1.2773542213111282\n",
      "x_new is  1.3286151012307559\n",
      "x is  1.3286151012307559\n",
      "x_new is  1.3324240221072825\n",
      "x is  1.3324240221072825\n",
      "x_new is  1.3759839419018691\n",
      "x is  1.3759839419018691\n",
      "x_new is  1.3762494519690722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11, 23)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random             \t# just for generating random mountains                                 \t \n",
    "\n",
    "# generate random mountains                                                                               \t \n",
    "\n",
    "w = [.05, random.random()/3, random.random()/3]\n",
    "h = [1.+math.sin(1+x/.6)*w[0]+math.sin(-.3+x/9.)*w[1]+math.sin(-.2+x/30.)*w[2] for x in range(100)]\n",
    "\n",
    "def climb(x, h):\n",
    "    # keep climbing until we've found a summit\n",
    "    summit = False\n",
    "\n",
    "    # edit here\n",
    "    for x_new in range(max(0, x-5), min(99, x + 5)):\n",
    "        if h[x_new] > h[x]:\n",
    "            print(\"x is \", h[x])\n",
    "            print(\"x_new is \", h[x_new])\n",
    "            x = x_new\n",
    "\n",
    "    while not summit:\n",
    "        summit = True         # stop unless there's a way up\n",
    "        for x_new in range(max(0, x-5), min(99, x + 5)):\n",
    "            if h[x_new] > h[x]:\n",
    "                print(\"x is \", h[x])\n",
    "                print(\"x_new is \", h[x_new])\n",
    "                x = x_new         # right is higher, go there\n",
    "                summit = False    # and keep going\n",
    "    return x\n",
    "\n",
    "\n",
    "def main(h):\n",
    "    # start at a random place                                                                                  \t \n",
    "    x0 = random.randint(1, 98)\n",
    "    x = climb(x0, h)\n",
    "\n",
    "    return x0, x\n",
    "\n",
    "main(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Probabilities\n",
    "Write a program that prints \"I love\" followed by one word: the additional word should be 'dogs' with 80% probability, 'cats' with 10% probability, and 'bats' with 10% probability.\n",
    "\n",
    "Here's an example output:\n",
    "\n",
    "I love bats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love cats\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def main():\n",
    "    prob_dogs = 0.8\n",
    "    prob_cats = 0.9\n",
    "    prob_bats = 1\n",
    "    probs = random.random()\n",
    "    if probs <= prob_dogs:\n",
    "        favourite = \"dogs\"\n",
    "    elif probs > prob_dogs and probs <= prob_cats:\n",
    "        favourite = \"cats\"\n",
    "    elif probs > prob_cats and probs <= prob_bats:\n",
    "        favourite = \"bats\"\n",
    "\n",
    "    print(\"I love \" + favourite) \n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Warm-up Temperature\n",
    "Suppose the current solution has score S_old = 150 and you try a small modification to create a new solution with score S_new = 140. In the greedy solution, this new solution wouldn't be accepted because it would mean a decrease in the score. In simulated annealing, the new solution is accepted with a certain probability as explained above.\n",
    "\n",
    "Modify the accept_prob function so that it returns the probability of accepting the new state using simulated annealing. The program should take the two score values (the current and the new) and the temperature value as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def accept_prob(S_old, S_new, T):\n",
    "    # this is the acceptance \"probability\" in the greedy hill-climbing method\n",
    "    # where new solutions are accepted if and only if they are better\n",
    "    # than the old one.\n",
    "    # change it to be the acceptance probability in simulated annealing\n",
    "\n",
    "    if S_new > S_old:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return math.exp(-(S_old-S_new)/T)\n",
    "\n",
    "\n",
    "# the above function will be used as follows. this is shown just for\n",
    "# your information; you don't have to change anything here\n",
    "def accept(S_old, S_new, T):\n",
    "    if random.random() < accept_prob(S_old, S_new, T):\n",
    "        print(True)\n",
    "    else:\n",
    "        print(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Simulated Annealing\n",
    "Let's use simulated annealing to solve a simple two-dimensional optimization problem. The following code runs 50 optimization tracks in parallel (at the same time). It currently only looks around the current solution and only accepts moves that go up. Modify the program so that it uses simulated annealing.\n",
    "\n",
    "Remember that the probability of accepting a solution that lowers the score is given by prob = exp(–(S_old - S_new)/T). Remember to also adjust the temperature in a way that it decreases as the simulation goes on, and to handle T=0 case correctly.\n",
    "\n",
    "Your goal is to ensure that on the average, at least 30 of the optimization tracks find the global optimum (the highest peak).\n",
    "\n",
    "If plotting the code takes too long, use this gist to plot the code locally on your computer. It should be significantly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v8/khccfr5948lgp_pl995rzy2w0000gn/T/ipykernel_26016/2537500876.py:43: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  prob = np.exp(-(S_old - S_new) / T)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "N = 100       # problem size is N x N\n",
    "steps = 3000  # number of iterations\n",
    "tracks = 50   # parallel search tracks\n",
    "\n",
    "# generate landscape with multiple local optima\n",
    "def generator(x, y, x0=0.0, y0=0.0):\n",
    "    return (np.sin((x/N-x0)*np.pi) +\n",
    "            np.sin((y/N-y0)*np.pi) +\n",
    "            0.07*np.cos(12*(x/N-x0)*np.pi) +\n",
    "            0.07*np.cos(12*(y/N-y0)*np.pi))\n",
    "\n",
    "x0 = np.random.random() - 0.5\n",
    "y0 = np.random.random() - 0.5\n",
    "h = np.fromfunction(np.vectorize(generator), (N, N), x0=x0, y0=y0, dtype=int)\n",
    "peak_x, peak_y = np.unravel_index(np.argmax(h), h.shape)\n",
    "\n",
    "# starting points\n",
    "x = np.random.randint(0, N, tracks)\n",
    "y = np.random.randint(0, N, tracks)\n",
    "\n",
    "def main():\n",
    "    global x, y\n",
    "\n",
    "    for step in range(steps):\n",
    "        # temperature schedule (linear cooling)\n",
    "        T = max(0, ((steps - step)/steps)**3-.005)\n",
    "\n",
    "        for i in range(tracks):\n",
    "            # propose neighbor\n",
    "            x_new = np.random.randint(max(0, x[i]-2), min(N, x[i]+3))\n",
    "            y_new = np.random.randint(max(0, y[i]-2), min(N, y[i]+3))\n",
    "\n",
    "            S_old = h[x[i], y[i]]\n",
    "            S_new = h[x_new, y_new]\n",
    "\n",
    "            # acceptance decision\n",
    "            if S_new >= S_old:\n",
    "                x[i], y[i] = x_new, y_new\n",
    "            else:\n",
    "                prob = np.exp(-(S_old - S_new) / T)\n",
    "                if np.random.rand() < prob:\n",
    "                    x[i], y[i] = x_new, y_new\n",
    "\n",
    "    # count how many tracks found the true global peak\n",
    "    print(sum([x[j] == peak_x and y[j] == peak_y for j in range(tracks)]))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Dealing with uncertainty\n",
    "## 2.1 Probability Fundamentals\n",
    "### Exercise 7: Flip the coin\n",
    "Write a program that generates 10000 random zeros and ones where the probability of one is p1 and the probability of zero is 1-p1 (hint: np.random.choice([0,1], p=[1-p1, p1], size=10000)), counts the number of occurrences of 5 consecutive ones (\"11111\") in the sequence, and outputs this number as a return value. Check that for p1 = 2/3, the count is close to 10000 x (2/3)^5 ≈ 1316.9.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate(p1):\n",
    "    # change this so that it generates 10000 random zeros and ones\n",
    "    # where the probability of one is p1\n",
    "    seq = np.random.choice([0,1], p=[1 - p1, p1], size = 10000)\n",
    "    return seq\n",
    "\n",
    "def count(seq):\n",
    "    # insert code to return the number of occurrences of 11111 in the sequence\n",
    "    ocurrences = 0\n",
    "\n",
    "    for i in range(len(seq) - 4):\n",
    "        if seq[i] == 1 and seq[i + 1] == 1 and seq[i + 2] == 1 and seq[i + 3] == 1 and seq[i + 4] == 1:\n",
    "            ocurrences += 1\n",
    "        \n",
    "    return ocurrences\n",
    "\n",
    "def main(p1):\n",
    "    seq = generate(p1)\n",
    "    return count(seq)\n",
    "\n",
    "print(main(2/3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Fishing in the Nordics\n",
    "Suppose we also happen to know the gender of the lottery winner. Write a function that uses the above numbers and tries to guess the nationality of the winner when we know that the winner is a fisher and their gender (either female or male).\n",
    "\n",
    "The argument of the function should be the gender of the winner ('female' or 'male'). The return value of the function should be a pair (country, probability) where country is the most likely nationality of the winner and probability is the probability of the country being the nationality of the winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if the winner is male, my guess is he's from Norway; probability 54.23%\n",
      "if the winner is female, my guess is she's from Iceland; probability 44.84%\n"
     ]
    }
   ],
   "source": [
    "countries = ['Denmark', 'Finland', 'Iceland', 'Norway', 'Sweden']\n",
    "populations = [5615000, 5439000, 324000, 5080000, 9609000]\n",
    "male_fishers = [1822, 2575, 3400, 11291, 1731]\n",
    "female_fishers = [69, 77, 400, 320, 26] \n",
    "\n",
    "def guess(winner_gender):\n",
    "    if winner_gender == 'female':\n",
    "        fishers = female_fishers\n",
    "    else:\n",
    "        fishers = male_fishers\n",
    "\n",
    "    # write your solution here\n",
    "    total = sum(fishers)\n",
    "    probs = []\n",
    "\n",
    "    for i in range(len(fishers)):\n",
    "        probability = (fishers[i]/total)*100\n",
    "        probs.append(probability)\n",
    "\n",
    "    guess = None\n",
    "    biggest = 0.0\n",
    "    for i in range(len(fishers)):\n",
    "        if probs[i] > biggest:\n",
    "            biggest = probs[i]\n",
    "            guess = countries[i]\n",
    "    return (guess, biggest)  \n",
    "\n",
    "def main():\n",
    "    country, fraction = guess(\"male\")\n",
    "    print(\"if the winner is male, my guess is he's from %s; probability %.2f%%\" % (country, fraction))\n",
    "    country, fraction = guess(\"female\")\n",
    "    print(\"if the winner is female, my guess is she's from %s; probability %.2f%%\" % (country, fraction))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The Bayes Rules\n",
    "### Exercise 9: Block or not\n",
    "Let's suppose you have a social media account on Instagram, Twitter, or some other platform. (Just in case you don't, it doesn't matter. We'll fill you in with the relevant information.) You check your account and notice that you have a new follower – this means that another user has decided to start following you to see things that you post. You don't recognize the person, and their username (or \"handle\" as it's called) is a little strange: John37330190. You don't want to have creepy bots following you, so you wonder. To decide whether you should block the new follower, you decide to use the Bayes rule!\n",
    "\n",
    "Suppose we know the probability that a new follower is a bot. You'll be writing a program that takes this value as an input. For now, let's just call this value P(bot). You'll also be given the probability that the username of a bot account includes an 8-digit number, which we'll call P(8-digits | bot), as well as the same probability for human (non-bot) accounts, P(8-digits | human).\n",
    "\n",
    "To use the Bayes rule, we'll also need to know the probability that a new follower (can be either bot or human) has an 8-digit number in their username, P(8-digits). The nice thing is that we can calculate P(8-digits) from the above information. The formula is as follows:\n",
    "\n",
    "P(8-digits) = P(8-digits | bot) x P(bot) + P(8-digits | human) x P(human)\n",
    "Remember that you can get P(human) simply as 1–P(bot), since these are the only options. (We consider business and other accounts as \"human\" as long as they aren't bots.)\n",
    "\n",
    "Write a program that takes as input the probability of a follower being a bot (pbot), the probability of a bot having a username with 8 digits (p8_bot), and the probability of a human having a username with 8 digits (p8_human). The values for these inputs are free for you to choose, but they have to be probabilitites, so they have to be between 0 and 1.\n",
    "\n",
    "Using the numbers you give the program calculate P(8-digits) and then use it and the Bayes rule to calculate and print out the probability of the new follower being a bot, P(bot | 8-digits):\n",
    "\n",
    "P(bot | 8-digits) =  P(8-digits | bot) x P(bot) / P(8-digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64\n"
     ]
    }
   ],
   "source": [
    "def bot8(pbot, p8_bot, p8_human):\n",
    "    # P(human) = 1 - P(bot)\n",
    "    phuman = 1 - pbot\n",
    "    \n",
    "    # total probability of having 8 digits\n",
    "    p8_total = p8_bot * pbot + p8_human * phuman\n",
    "    \n",
    "    # Bayes rule\n",
    "    pbot_8 = (p8_bot * pbot) / p8_total\n",
    "    \n",
    "    print(pbot_8)\n",
    "\n",
    "# Example values\n",
    "pbot = 0.1       # 10% of new followers are bots\n",
    "p8_bot = 0.8     # 80% of bots have 8 digits in username\n",
    "p8_human = 0.05  # 5% of humans have 8 digits in username\n",
    "\n",
    "bot8(pbot, p8_bot, p8_human)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Naive Bayes Classifier\n",
    "### Exercise 10: Naive Bayes classifier\n",
    "We have two dice in our desk drawer. One is a normal, plain die with six sides such that each of the sides comes up with equal 1/6 probability. The other one is a loaded die that also has six sides, but that however gives the outcome 6 with every second try on the average, the other five sides being equally probable.\n",
    "\n",
    "Thus with the first, normal die the probabilities of each side are the same, 0.167 (or 16.7 %). With the second, loaded die, the probability of 6 is 0.5 (or 50 %) and each of the other five sides has probability 0.1 (or 10 %).\n",
    "\n",
    "The following program gets as its input the choice of the die and then simulates a sequence of ten rolls.\n",
    "\n",
    "Your task: starting from the odds 1:1, use the naive Bayes method to update the odds after each outcome to decide which of the dice is more likely. Edit the function bayes so that it returns True if the most likely die is the loaded one, and False otherwise. Remember to be careful with the indices when accessing list elements!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling a loaded die\n",
      "rolled 1\n",
      "rolled 4\n",
      "rolled 1\n",
      "rolled 6\n",
      "rolled 1\n",
      "rolled 1\n",
      "rolled 2\n",
      "rolled 6\n",
      "rolled 4\n",
      "rolled 5\n",
      "I think normal\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p1 = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]   # normal\n",
    "p2 = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]   # loaded\n",
    "\n",
    "def roll(loaded):\n",
    "    if loaded:\n",
    "        print(\"rolling a loaded die\")\n",
    "        p = p2\n",
    "    else:\n",
    "        print(\"rolling a normal die\")\n",
    "        p = p1\n",
    "\n",
    "    # roll the dice 10 times\n",
    "    # add 1 to get dice rolls from 1 to 6 instead of 0 to 5\n",
    "    sequence = np.random.choice(6, size=10, p=p) + 1 \n",
    "    for roll in sequence:\n",
    "        print(\"rolled %d\" % roll)\n",
    "        \n",
    "    return sequence\n",
    "\n",
    "def bayes(sequence):\n",
    "    odds = 1.0           # start with odds 1:1\n",
    "    for roll in sequence:\n",
    "        # edit here to update the odds\n",
    "        likelihood = p2[roll - 1]/p1[roll - 1]\n",
    "        odds *= likelihood\n",
    "    if odds > 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "sequence = roll(True)\n",
    "if bayes(sequence):\n",
    "    print(\"I think loaded\")\n",
    "else:\n",
    "    print(\"I think normal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Machine Learning\n",
    "## 3.1 Linear Regression\n",
    "### Exercise 11: Real estate price predictions\n",
    "Edit the following program so that it can process multiple cabins that may be described by any number of details (like five below), at the same time. You can assume that each of the lists contained in the list x and the coefficients c contain the same number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258250\n",
      "76100\n",
      "492750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[258250, 76100, 492750]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input values for three mökkis: size, size of sauna, distance to water, number of indoor bathrooms, \n",
    "# proximity of neighbors\n",
    "X = np.array([[66, 5, 15, 2, 500], \n",
    "     [21, 3, 50, 1, 100], \n",
    "     [120, 15, 5, 2, 1200]])\n",
    "c = np.array([3000, 200, -50, 5000, 100])    # coefficient values\n",
    "\n",
    "def predict(X, c):\n",
    "    prices = []\n",
    "    \n",
    "    for cabin in X:\n",
    "        price = 0\n",
    "        for i in range(len(cabin)):\n",
    "            price += cabin[i]*c[i]\n",
    "        \n",
    "        prices.append(price)\n",
    "        print(price)     \n",
    "    return prices  \n",
    "    \n",
    "\n",
    "predict(X, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12: Least squares\n",
    "Write a program that calculates the squared error for multiple sets of coefficient values and prints out the index of the set that yields the smallest squared error: this is a poor man's version of the least squares method where we only consider a fixed set of alternative coefficient vectors instead of finding the global optimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best set is set 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data\n",
    "X = np.array([[66, 5, 15, 2, 500], \n",
    "              [21, 3, 50, 1, 100], \n",
    "              [120, 15, 5, 2, 1200]])\n",
    "y = np.array([250000, 60000, 525000])\n",
    "\n",
    "# alternative sets of coefficient values\n",
    "c = np.array([[3000, 200 , -50, 5000, 100], \n",
    "              [2000, -250, -100, 150, 250], \n",
    "              [3000, -100, -150, 0, 150]])   \n",
    "\n",
    "def find_best(X, y, c):\n",
    "    smallest_error = np.Inf\n",
    "    best_index = -1\n",
    "    for i, coeff in enumerate(c):\n",
    "        predictions = X @ coeff\n",
    "        error = np.sum((y - predictions)**2)\n",
    "        \n",
    "        if error < smallest_error:\n",
    "            smallest_error = error\n",
    "            best_index = i\n",
    "          # edit here: calculate the sum of squared error with coefficient set coeff and\n",
    "                 # keep track of the one yielding the smallest squared error\n",
    "    print(\"the best set is set %d\" % best_index)\n",
    "\n",
    "\n",
    "find_best(X, y, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13: Predictions with more data\n",
    "Write a program that reads cabin details and prices from a CSV file (a standard format for tabular data) and fits a linear regression model to it. The program should be able to handle any number of data points (cabins) described by any number of features (like size, size of sauna, number of bathrooms, ...).\n",
    "\n",
    "You can read a CSV file with the function np.genfromtxt(datafile, skip_header=1). This will return a numpy array that contains the feature data in the columns preceding the last one, and the price data in the last column. The option skip_header=1 just means that the first line in the file is supposed to contain just the column names and shouldn't be included in the actual data.\n",
    "\n",
    "The output of the program should be the estimated coefficients and the predicted or \"fitted\" prices for the same set of cabins used to estimate the parameters. So if you fit the model using data for six cabins with known prices, the program will print out the prices that the model predicts for those six cabins (even if the actual prices are already given in the data).\n",
    "\n",
    "Note that here we will actually only simulate the file input using Python's io.StringIO function that takes an input string and pretends that the contents is coming from a file. In practice, you would just name the input file that contains the data in the same format as the string input below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2989.6  800.6  -44.8 3890.8   99.8]\n",
      "[127907.6 222269.8 143604.5 268017.6 460686.6 406959.9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v8/khccfr5948lgp_pl995rzy2w0000gn/T/ipykernel_26016/1110047559.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  c = np.linalg.lstsq(x,y)[0]  # coefficients of the linear regression\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "input_string = '''\n",
    "25 2 50 1 500 127900\n",
    "39 3 10 1 1000 222100\n",
    "13 2 13 1 1000 143750\n",
    "82 5 20 2 120 268000\n",
    "130 6 10 2 600 460700\n",
    "115 6 10 1 550 407000\n",
    "'''\n",
    "\n",
    "np.set_printoptions(precision=1)    # this just changes the output settings for easier reading\n",
    " \n",
    "\n",
    "def fit_model(input_file):\n",
    "    # Please write your code inside this function\n",
    "    data = np.genfromtxt(input_file)\n",
    "    x = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "    # read the data in and fit it. the values below are placeholder values\n",
    "    c = np.linalg.lstsq(x,y)[0]  # coefficients of the linear regression\n",
    "    \n",
    "\n",
    "    print(c)\n",
    "    print(x @ c)\n",
    "\n",
    "# simulate reading a file\n",
    "input_file = StringIO(input_string)\n",
    "fit_model(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14: Training data vs test data\n",
    "Write a program that reads data about one set of cabins (training data), estimates linear regression coefficients based on it, then reads data about another set of cabins (test data), and predicts the prices in it. Note that both data sets contain the actual prices, but the program should ignore the prices in the second set. They are given only for comparison.\n",
    "\n",
    "You can read the data into the program the same way as in the previous exercise.\n",
    "\n",
    "You should then separate the feature and price data that you have just read from the file into two separate arrays names x_train and y_train, so that you can use them as argument to np.linalg.lstsq.\n",
    "\n",
    "The program should work even if the number of features used to describe the cabins differs from five (as long as the same number of features are given in each file).\n",
    "\n",
    "The output should be the set of coefficients for the linear regression and the predicted prices for the second set of cabins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198102.4 289108.3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    np.set_printoptions(precision=1)    # this just changes the output settings for easier reading\n",
    "    x_train = np.array(\n",
    "        [\n",
    "            [25, 2, 50, 1, 500], \n",
    "            [39, 3, 10, 1, 1000], \n",
    "            [13, 2, 13, 1, 1000], \n",
    "            [82, 5, 20, 2, 120], \n",
    "            [130, 6, 10, 2, 600],\n",
    "            [115, 6, 10, 1, 550 ]\n",
    "        ]\n",
    "    )   \n",
    "    \n",
    "    y_train = np.array([127900, 222100, 143750, 268000, 460700, 407000])\n",
    "\n",
    "    # add the feature data for the two new cabins here. note: don't include the price data\n",
    "    x_test = np.array([\n",
    "        [36,3,15,1,850], [75,5,18,2,540]\n",
    "            ])\n",
    "\n",
    "    c = np.linalg.lstsq(x_train, y_train, rcond=-1)[0]\n",
    "\n",
    "    # this will print the predicted prices for the six cabins in the training data\n",
    "    # change this so that it predicts the prices of the two new cabins that are not\n",
    "    # included in the training set\n",
    "\n",
    "    print(x_test @ c)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The nearest neighbor method\n",
    "### Exercise 15: Vector distances\n",
    "You are given an array x_train with multiple input vectors (the \"training data\") and another array x_test with one more input vector (the \"test data\"). Find the vector in x_train that is most similar to the vector in x_test. In other words, find the nearest neighbor of the test data point x_test.\n",
    "\n",
    "\n",
    "The code template gives the function dist to calculate the distance between any two vectors. What you need to add is the implementation of the function nearest that takes the arrays x_train and x_test and prints the index (as an integer between 0, ..., len(x_train)-1) of the nearest neighbor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.random.rand(10, 3)   # generate 10 random vectors of dimension 3\n",
    "x_test = np.random.rand(3)        # generate one more random vector of the same dimension\n",
    "\n",
    "def dist(a, b):\n",
    "    sum = 0\n",
    "    for ai, bi in zip(a, b):\n",
    "        sum = sum + (ai - bi)**2\n",
    "    return np.sqrt(sum)\n",
    "    \n",
    "def nearest(x_train, x_test):\n",
    "    nearest_index = -1\n",
    "    min_distance = np.Inf\n",
    "    \n",
    "    # Loop through all vectors in x_train\n",
    "    for i in range(len(x_train)):\n",
    "        # Calculate distance between current training vector and test vector\n",
    "        current_distance = dist(x_train[i], x_test)\n",
    "        \n",
    "        # Update if this is the smallest distance found so far\n",
    "        if current_distance < min_distance:\n",
    "            min_distance = current_distance\n",
    "            nearest_index = i\n",
    "    print(nearest_index)\n",
    "    return nearest_index\n",
    "\n",
    "# Test the function\n",
    "nearest(x_train, x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16: Nearest neighbor\n",
    "In the basic nearest neighbor classifier, the only thing that matters is the class label of the nearest neighbor. But the nearest neighbor may sometimes be noisy or otherwise misleading. Therefore, it may be better to also consider the other nearby data points in addition to the nearest neighbor.\n",
    "\n",
    "This idea leads us to the so called k-nearest neighbor method, where we consider all the k nearest neighbors. If k=3, for example, we'd take the three nearest points and choose the class label based on the majority class among them.\n",
    "\n",
    "The program below uses the library sklearn to generate a random dataset. You don't need to be familiar with sklearn, we explain all the necessary information below. Each sample in the dataset has two input features (X) and one binary output class (y). We can think of a sample as a cabin, with its size and price as its input features, and whether we like it (1) or not (0) as its output class.\n",
    "\n",
    "The program first generates the random dataset and splits it into training and test sets. Then, for each cabin in the test set, it identifies its nearest neighbor (k=1) from the cabins in the train set using the distance function. However, the program has very high standards and dislikes all the cabins y_predict[i] = 0.\n",
    "\n",
    "Your goal is to make the program smarter by predicting the output class (y_predict) for each cabin in the test set based on the majority output class (y_train) of its three nearest neighbor (k=3).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# create random data with two classes\n",
    "X, Y = make_blobs(n_samples=16, n_features=2, centers=2, center_box=(-2, 2))\n",
    "\n",
    "# scale the data so that all values are between 0.0 and 1.0\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# split two data points from the data as test data and\n",
    "# use the remaining n-2 points as the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=2)\n",
    "\n",
    "# place-holder for the predicted classes\n",
    "y_predict = np.empty(len(y_test), dtype=np.int64)\n",
    "\n",
    "# produce line segments that connect the test data points\n",
    "# to the nearest neighbors for drawing the chart\n",
    "lines = []\n",
    "\n",
    "# distance function\n",
    "def dist(a, b):\n",
    "    sum = 0\n",
    "    for ai, bi in zip(a, b):\n",
    "        sum = sum + (ai - bi)**2\n",
    "    return np.sqrt(sum)\n",
    "\n",
    "\n",
    "def main(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    global y_predict\n",
    "    global lines\n",
    "\n",
    "    k = 3    # classify our test items based on the classes of 3 nearest neighbors\n",
    "\n",
    "    # process each of the test data points\n",
    "    for i, test_item in enumerate(X_test):\n",
    "        # calculate the distances to all training points\n",
    "        distances = [dist(train_item, test_item) for train_item in X_train]\n",
    "\n",
    "        # find indices of k nearest neighbors\n",
    "        nearest_indices = np.argsort(distances)[:k]\n",
    "\n",
    "        # get the labels of the nearest neighbors\n",
    "        neighbor_labels = y_train[nearest_indices]\n",
    "\n",
    "        # majority vote\n",
    "        values, counts = np.unique(neighbor_labels, return_counts=True)\n",
    "        majority_label = values[np.argmax(counts)]\n",
    "\n",
    "        # store prediction\n",
    "        y_predict[i] = majority_label\n",
    "\n",
    "        # connect to nearest neighbor (or all k if you want)\n",
    "        nearest = nearest_indices[0]\n",
    "\n",
    "        # create a line connecting the points for the chart\n",
    "        # you may change this to do the same for all the k nearest neigbhors if you like\n",
    "        # but it will not be checked in the tests\n",
    "        lines.append(np.stack((test_item, X_train[nearest])))\n",
    "\n",
    "    print(y_predict)\n",
    "\n",
    "main(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Working with Text\n",
    "### Exercise 17: Bag of words\n",
    "Your task is to write a function that calculates the distances (or differences) between a pair of lines in the This Little Piggy rhyme.\n",
    "\n",
    "Every row in the list data represents one line in the rhyme.\n",
    "\n",
    "When you run the code, you see that the output of the whole program is a list of lists. When your function works correctly, each list will contain the distances between a single row and all the other rows in data.\n",
    "\n",
    "Note that the program will compare every row also with itself. In this case – when the compared rows are the same – their distance will be zero.\n",
    "\n",
    "You can use the function abs(x-y) to calculate the distance between numbers x and y, where x comes from list row1 and y comes from row2.\n",
    "\n",
    "Your program must work with any text, not only with the rhyme This Little Piggy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 5, 6, 5, 12], [5, 0, 5, 4, 9], [6, 5, 0, 3, 12], [5, 4, 3, 0, 11], [12, 9, 12, 11, 0]]\n"
     ]
    }
   ],
   "source": [
    "# this data here is the bag of words representation of This Little Piggy\n",
    "data = [[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 1, 1, 0, 1, 3, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]]\n",
    "\n",
    "def distance(row1, row2):\n",
    "    # fix this function so that it returns \n",
    "    # the sum of differences between the occurrences\n",
    "    # of each word in row1 and row2.\n",
    "    # you can assume that row1 and row2 are lists with equal length, containing numeric values.\n",
    "    diff = []\n",
    "\n",
    "    for i in range(len(row1) -1):\n",
    "        diff.append(row1[i]-row2[i])\n",
    "    \n",
    "    difference = sum(np.abs(diff))\n",
    "    return difference\n",
    "\n",
    "\n",
    "def all_pairs(data):\n",
    "    # this calls the distance function for all the two-row combinations in the data\n",
    "    # you do not need to change this\n",
    "    dist = [[distance(sent1, sent2) for sent1 in data] for sent2 in data]\n",
    "    print(dist)\n",
    "\n",
    "all_pairs(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18: TF-IDF\n",
    "Modify the following program to print out the tf-idf values for each document and each word. The following code calculates the tf and df values, so you'll just need to combine them according to the correct formula. There are three documents (sentences) and a total of eight terms (unique words), so the output should be three lists of eight tf-idf values each.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "text = '''Humpty Dumpty sat on a wall\n",
    "Humpty Dumpty had a great fall\n",
    "all the king's horses and all the king's men\n",
    "couldn't put Humpty together again'''\n",
    "\n",
    "def main(text):\n",
    "    # tasks your code should perform:\n",
    "    \n",
    "    # 1. split the text into words, and get a list of unique words that appear in it\n",
    "    # a short one-liner to separate the text into sentences (with words lower-cased to make words equal \n",
    "    # despite casing) can be done with \n",
    "\n",
    "    docs = [line.lower().split() for line in text.split('\\n')]\n",
    "\n",
    "    # 2. go over each unique word and calculate its term frequency, and its document frequency\n",
    "\n",
    "    N = len(docs)\n",
    "\n",
    "    # create the vocabulary: the list of words that appear at least once\n",
    "    vocabulary = list(set(text.split()))\n",
    "\n",
    "    df = {}\n",
    "    tf = {}\n",
    "    for word in vocabulary:\n",
    "        # tf: number of occurrences of word w in document divided by document length\n",
    "        # note: tf[word] will be a list containing the tf of each word for each document\n",
    "        # for example tf['he'][0] contains the term frequence of the word 'he' in the first\n",
    "        # document\n",
    "        tf[word] = [doc.count(word)/len(doc) for doc in docs]\n",
    "\n",
    "        # df: number of documents containing word w\n",
    "        df[word] = sum([word in doc for doc in docs])/N\n",
    "    # loop through documents to calculate the tf-idf values\n",
    "    # 3. after you have your term frequencies and document frequencies, go over each line in the text and \n",
    "    # calculate its TF-IDF representation, which will be a vector\n",
    "\n",
    "    tfidf_matrix = []\n",
    "    for doc_index, doc in enumerate(docs):\n",
    "        tfidf = []\n",
    "        for word in vocabulary:\n",
    "            if df[word] > 0:\n",
    "                idf = math.log10(N/df[word])\n",
    "            else:\n",
    "                idf = 0\n",
    "            tfidf_value = tf[word][doc_index] * idf\n",
    "            tfidf.append(tfidf_value)\n",
    "        tfidf_matrix.append(tfidf) \n",
    "\n",
    "    tfidf_matrix = np.array(tfidf_matrix)\n",
    "\n",
    "    # 4. after you have calculated the TF-IDF representations for each line in the text, you need to\n",
    "    # calculate the distances between each line to find which are the closest.\n",
    "\n",
    "    num_docs = len(tfidf_matrix)\n",
    "    dist = np.full((num_docs, num_docs), np.inf, dtype=float)  # fill diag with inf\n",
    "    for i in range(num_docs):\n",
    "        for j in range(num_docs):\n",
    "            if i != j:\n",
    "                dist[i, j] = np.sum(np.abs(tfidf_matrix[i] - tfidf_matrix[j]))\n",
    "\n",
    "    min_idx = np.unravel_index(np.argmin(dist), dist.shape)\n",
    "    print(min_idx)\n",
    "\n",
    "main(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Overfitting\n",
    "### Exercise 19: Looking out for overfitting\n",
    "The program below uses the k-nearest neighbors algorithm. The idea is to not only look at the single nearest training data point (neighbor) but for example the five nearest points, if k=5. The normal nearest neighbor classifier amounts to using k=1.\n",
    "\n",
    "Write a program that does the classification for some value of k and prints out the training and testing accuracy.\n",
    "\n",
    "Hint: You can get the model accuracy for a given set using the function knn.score.\n",
    "\n",
    "Try different values of k to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 1.000000\n",
      "testing accuracy: 0.860606\n",
      "training accuracy: 0.937313\n",
      "testing accuracy: 0.860606\n",
      "training accuracy: 0.934328\n",
      "testing accuracy: 0.890909\n",
      "training accuracy: 0.934328\n",
      "testing accuracy: 0.878788\n",
      "training accuracy: 0.928358\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.928358\n",
      "testing accuracy: 0.896970\n",
      "training accuracy: 0.931343\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.896970\n",
      "training accuracy: 0.931343\n",
      "testing accuracy: 0.903030\n",
      "training accuracy: 0.934328\n",
      "testing accuracy: 0.903030\n",
      "training accuracy: 0.937313\n",
      "testing accuracy: 0.903030\n",
      "training accuracy: 0.940299\n",
      "testing accuracy: 0.896970\n",
      "training accuracy: 0.937313\n",
      "testing accuracy: 0.903030\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.890909\n",
      "training accuracy: 0.931343\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.931343\n",
      "testing accuracy: 0.903030\n",
      "training accuracy: 0.928358\n",
      "testing accuracy: 0.903030\n",
      "training accuracy: 0.931343\n",
      "testing accuracy: 0.896970\n",
      "training accuracy: 0.928358\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.934328\n",
      "testing accuracy: 0.896970\n",
      "training accuracy: 0.928358\n",
      "testing accuracy: 0.903030\n",
      "training accuracy: 0.937313\n",
      "testing accuracy: 0.903030\n",
      "training accuracy: 0.934328\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.931343\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.928358\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.934328\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.934328\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.931343\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.928358\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.931343\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.931343\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.928358\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.928358\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.928358\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.922388\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.922388\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.922388\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.919403\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.925373\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.922388\n",
      "testing accuracy: 0.903030\n",
      "training accuracy: 0.922388\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.922388\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.922388\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.922388\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.919403\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.919403\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.916418\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.913433\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.913433\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.913433\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.913433\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.913433\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.910448\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.910448\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.913433\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.910448\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.907463\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.904478\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.907463\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.904478\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.901493\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.901493\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.904478\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.895522\n",
      "testing accuracy: 0.921212\n",
      "training accuracy: 0.895522\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.895522\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.895522\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.895522\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.889552\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.892537\n",
      "testing accuracy: 0.915152\n",
      "training accuracy: 0.880597\n",
      "testing accuracy: 0.909091\n",
      "training accuracy: 0.880597\n",
      "testing accuracy: 0.903030\n",
      "training accuracy: 0.874627\n",
      "testing accuracy: 0.896970\n",
      "training accuracy: 0.880597\n",
      "testing accuracy: 0.896970\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# do not edit this\n",
    "# create fake data\n",
    "x, y = make_moons(\n",
    "    n_samples=500,  # the number of observations\n",
    "    random_state=42,\n",
    "    noise=0.3\n",
    ")\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "best_k = None\n",
    "best_test_acc = 0.0\n",
    "results = []\n",
    "\n",
    "# Create a classifier and fit it to our data\n",
    "for i in range(1,100):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(x_train, y_train)\n",
    "\n",
    "    train_acc = knn.score(x_train, y_train)\n",
    "    test_acc = knn.score(x_test, y_test)\n",
    "    results.append((i, train_acc, test_acc))\n",
    "    \n",
    "    if test_acc > best_test_acc:\n",
    "        best_i = i\n",
    "        best_test_acc = test_acc\n",
    "\n",
    "    print(\"training accuracy: %f\" % train_acc)\n",
    "    print(\"testing accuracy: %f\" % test_acc)\n",
    "    \n",
    "\n",
    "print(best_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 - Neural Networks\n",
    "## 4.1 Logistic Regression\n",
    "### Exercise 20: Logistic regression\n",
    "You are given a set of three input values and you also have multiple alternative sets of three coefficients. Calculate the predicted output value using the linear formula combined with the logistic activation function.\n",
    "\n",
    "Do this with all the alternative sets of coefficients. Which of the coefficient sets yields the highest sigmoid output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1544652650835347\n",
      "0.45016600268752216\n",
      "0.8455347349164652\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([4, 3, 0])\n",
    "c1 = np.array([-.5, .1, .08])\n",
    "c2 = np.array([-.2, .2, .31])\n",
    "c3 = np.array([.5, -.1, 2.53])\n",
    "\n",
    "def sigmoid(z):\n",
    "    # add your implementation of the sigmoid function here\n",
    "    a =  1 / (1+np.exp(-z))\n",
    "    print(a)\n",
    "    return(a)\n",
    "\n",
    "# calculate the output of the sigmoid for x with all three coefficients\n",
    "z1 = np.dot(x,c1)\n",
    "z2 = np.dot(x,c2)\n",
    "z3 = np.dot(x,c3)\n",
    "\n",
    "s1 = sigmoid(z1)\n",
    "s2 = sigmoid(z2)\n",
    "s3 = sigmoid(z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 From logistic regression to neural networks\n",
    "### Exercise 21: Neural networks\n",
    "We have trained a simple neural network with a larger set of cabin price data. The network predicts the price of the cabin based on the attributes of the cabin. The network consists of an input layer with five nodes, a hidden layer with two nodes, a second hidden layer with two nodes, and finally an output layer with a single node. In addition, there is a single bias node for each hidden layer and the output layer.\n",
    "\n",
    "The program below uses the weights of this trained network to perform what is called a forward pass of the neural network. The forward pass is running the input variables through the neural network to obtain output, in this case the price of a cabin of given attributes.\n",
    "\n",
    "The program is incomplete though. The program only does the forward pass up to the first hidden layer and is missing the second hidden layer and the output layer.\n",
    "\n",
    "Modify the program to do a full forward pass and print out the price prediction. To do this, write out the remaining forward pass operations and use the ReLU activation function for the hidden nodes, and a linear (identity) activation for the output node. ReLU activation function returns either the input value of the function, or zero, whichever is the largest, and linear activation just returns the input as output. After these are done, get a prediction for the price of a cabin which is described by the following feature vector [82, 2, 65, 3, 516]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 257136.43628059432\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w0 = np.array([[ 1.19627687e+01,  2.60163283e-01],\n",
    "               [ 4.48832507e-01,  4.00666119e-01],\n",
    "               [-2.75768443e-01,  3.43724167e-01],\n",
    "               [ 2.29138536e+01,  3.91783025e-01],\n",
    "               [-1.22397711e-02, -1.03029800e+00]])\n",
    "\n",
    "w1 = np.array([[11.5631751 , 11.87043684],\n",
    "               [-0.85735419,  0.27114237]])\n",
    "\n",
    "w2 = np.array([[11.04122165],\n",
    "               [10.44637262]])\n",
    "\n",
    "b0 = np.array([-4.21310294, -0.52664488])\n",
    "b1 = np.array([-4.84067881, -4.53335139])\n",
    "b2 = np.array([-7.52942418])\n",
    "\n",
    "x = np.array([[111, 13, 12, 1, 161],\n",
    "              [125, 13, 66, 1, 468],\n",
    "              [46, 6, 127, 2, 961],\n",
    "              [80, 9, 80, 2, 816],\n",
    "              [33, 10, 18, 2, 297],\n",
    "              [85, 9, 111, 3, 601],\n",
    "              [24, 10, 105, 2, 1072],\n",
    "              [31, 4, 66, 1, 417],\n",
    "              [56, 3, 60, 1, 36],\n",
    "              [49, 3, 147, 2, 179]])\n",
    "\n",
    "y = np.array([335800., 379100., 118950., 247200., 107950.,\n",
    "              266550.,  75850., 93300., 170650., 149000.])\n",
    "\n",
    "\n",
    "# Activation functions\n",
    "def hidden_activation(z):\n",
    "    # ReLU\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def output_activation(z):\n",
    "    # Linear activation\n",
    "    return z\n",
    "\n",
    "\n",
    "# Test input\n",
    "x_test = [[82, 2, 65, 3, 516]]\n",
    "\n",
    "for item in x_test:\n",
    "    # First hidden layer\n",
    "    h1_in = np.dot(item, w0) + b0\n",
    "    h1_out = hidden_activation(h1_in)\n",
    "\n",
    "    # Second hidden layer\n",
    "    h2_in = np.dot(h1_out, w1) + b1\n",
    "    h2_out = hidden_activation(h2_in)\n",
    "\n",
    "    # Output layer\n",
    "    out_in = np.dot(h2_out, w2) + b2\n",
    "    y_pred = output_activation(out_in)\n",
    "\n",
    "    print(\"Prediction:\", y_pred[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
